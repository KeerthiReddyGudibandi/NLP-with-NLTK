{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification\n",
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "V6kotndil-JT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHPLrU1iljo6",
        "outputId": "e7357ae4-9f61-4d81-8fff-4fc62caf0b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.81\n",
            "Most Informative Features\n",
            "   contains(outstanding) = True              pos : neg    =     11.1 : 1.0\n",
            "         contains(mulan) = True              pos : neg    =      9.0 : 1.0\n",
            "   contains(wonderfully) = True              pos : neg    =      8.6 : 1.0\n",
            "        contains(seagal) = True              neg : pos    =      8.2 : 1.0\n",
            "          contains(lame) = True              neg : pos    =      6.3 : 1.0\n",
            "         contains(damon) = True              pos : neg    =      6.1 : 1.0\n",
            "         contains(awful) = True              neg : pos    =      5.8 : 1.0\n",
            "         contains(flynt) = True              pos : neg    =      5.6 : 1.0\n",
            "        contains(wasted) = True              neg : pos    =      5.3 : 1.0\n",
            "          contains(jedi) = True              pos : neg    =      5.3 : 1.0\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "\n",
        "# Load movie reviews dataset\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Create a list of documents (each document is a list of words and a category label)\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Define a feature extractor function\n",
        "def document_features(document):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[f'contains({word})'] = (word in words)\n",
        "    return features\n",
        "\n",
        "# Create a frequency distribution of words\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "word_features = list(all_words)[:2000]  # Use the top 2000 words as features\n",
        "\n",
        "# Apply the feature extractor to each document\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Naive Bayes Accuracy:\", accuracy(classifier, test_set))\n",
        "\n",
        "# Show the most informative features\n",
        "classifier.show_most_informative_features(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees"
      ],
      "metadata": {
        "id": "MuuJ3LgbmTPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "from nltk.classify import DecisionTreeClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "\n",
        "# Load movie reviews dataset\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Create a list of documents (each document is a list of words and a category label)\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Define a feature extractor function\n",
        "def document_features(document):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[f'contains({word})'] = (word in words)\n",
        "    return features\n",
        "\n",
        "# Create a frequency distribution of words\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "word_features = list(all_words)[:2000]  # Use the top 2000 words as features\n",
        "\n",
        "# Apply the feature extractor to each document\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "\n",
        "# Train the Decision Tree classifier\n",
        "classifier = DecisionTreeClassifier.train(train_set, entropy_cutoff=0.05, support_cutoff=10)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Decision Tree Accuracy:\", accuracy(classifier, test_set))\n",
        "\n",
        "# Show the most informative features (if supported)\n",
        "try:\n",
        "    classifier.show_most_informative_features(10)\n",
        "except:\n",
        "    print(\"Decision Trees don't have a built-in method to show informative features.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRNk57Qvl3IG",
        "outputId": "b7df13a5-2abf-4f8d-fe31-8f3fa7386353"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.68\n",
            "Decision Trees don't have a built-in method to show informative features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Syntax and Parsing:\n",
        "## Dependency Parser"
      ],
      "metadata": {
        "id": "SWc9Nw03moVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement Dependency Parser to generate the parse tree\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "#Print dependency parse tree\n",
        "for token in doc:\n",
        "  print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "        [child for child in token.children])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rubQuvBhmrD2",
        "outputId": "3acde7b2-a6ff-4af9-8cb0-e7e3aaa4cdef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The det fox NOUN []\n",
            "quick amod fox NOUN []\n",
            "brown amod fox NOUN []\n",
            "fox nsubj jumps VERB [The, quick, brown]\n",
            "jumps ROOT jumps VERB [fox, over, .]\n",
            "over prep jumps VERB [dog]\n",
            "the det dog NOUN []\n",
            "lazy amod dog NOUN []\n",
            "dog pobj over ADP [the, lazy]\n",
            ". punct jumps VERB []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Chunking using Shallow parsing"
      ],
      "metadata": {
        "id": "FrW_Ig3vn1q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "chunk_grammar = r\"\"\"\n",
        "     NP: {<DT|JJ|NN.*>+}\n",
        "     PP: {<IN><NP>}\n",
        "     VP: {<VB.*><NP|PP|CLAUSE>+$}\n",
        "     CLAUSE: {<NP><VP>}\n",
        "\"\"\"\n",
        "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
        "parsed_tree = chunk_parser.parse(tagged_tokens)\n",
        "print(parsed_tree)\n",
        "parsed_tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOoSksoAn0O6",
        "outputId": "d30abb89-e1de-4a60-b63e-4b0396ef258c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN fox/NN)\n",
            "  jumps/VBZ\n",
            "  (PP over/IN (NP the/DT lazy/JJ dog/NN))\n",
            "  ./.)\n",
            "                                 S                                             \n",
            "     ____________________________|____________________________                  \n",
            "    |      |            |                                     PP               \n",
            "    |      |            |                         ____________|_____            \n",
            "    |      |            NP                       |                  NP         \n",
            "    |      |     _______|________________        |       ___________|______     \n",
            "jumps/VBZ ./. The/DT quick/JJ brown/NN fox/NN over/IN the/DT     lazy/JJ dog/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regex Parser"
      ],
      "metadata": {
        "id": "miAGxYlDuWQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import RegexpParser\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize and perform POS tagging\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged = pos_tag(tokens)\n",
        "print(\"Tagged Sentence:\", tagged)\n",
        "\n",
        "# Define the grammar for a simple noun phrase (NP)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# Create a regex parser with the defined grammar\n",
        "cp = RegexpParser(grammar)\n",
        "\n",
        "# Parse the sentence using the regex parser\n",
        "parsed_sentence = cp.parse(tagged)\n",
        "print(\"Parsed Sentence:\")\n",
        "print(parsed_sentence)\n",
        "\n",
        "# Visualize the parsed tree (optional)\n",
        "parsed_sentence.pprint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmw-oS6ZuDrZ",
        "outputId": "a083cba3-7136-495f-9176-0cce1e2404c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged Sentence: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
            "Parsed Sentence:\n",
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN)\n",
            "  ./.)\n",
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maximum entropy classifier"
      ],
      "metadata": {
        "id": "OgDscE-iu7RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Maximum entropy classifier\n",
        "import nltk\n",
        "from nltk.classify import MaxentClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "class MaxEntClassifier:\n",
        "  def __init__(self):\n",
        "    self.classifier= None\n",
        "  def train(self,labeled_featuresets,algorithm='GIS',max_iter=10):\n",
        "    self.classifier=MaxentClassifier.train(labeled_featuresets,algorithm=algorithm,max_iter=max_iter)\n",
        "  def classify(self,featureset):\n",
        "    return self.classifier.classify(featureset)\n",
        "  def accuracy(self,test_data):\n",
        "    return nltk.classify.accuracy(test_data)\n",
        "training_data=[\n",
        "    ({'feature1':10,'feature2':5},'class1'),\n",
        "    ({'feature1':3,'feature2':8},'class2')\n",
        "]\n",
        "classifier=MaxEntClassifier()\n",
        "classifier.train(training_data)\n",
        "test_data={'feature1':12,'feature2':3}\n",
        "predicted_class=classifier.classify(test_data)\n",
        "print(\"The predicted class \",predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8pCgKgnu75r",
        "outputId": "9179aea0-8915-45ae-c4a3-bfa9a830fa9c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (10 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.500\n",
            "             2          -0.33422        1.000\n",
            "             3          -0.21130        1.000\n",
            "             4          -0.15255        1.000\n",
            "             5          -0.11874        1.000\n",
            "             6          -0.09695        1.000\n",
            "             7          -0.08179        1.000\n",
            "             8          -0.07067        1.000\n",
            "             9          -0.06217        1.000\n",
            "         Final          -0.05548        1.000\n",
            "The predicted class  class2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leftmost and Rightmost Derivation"
      ],
      "metadata": {
        "id": "MsJlL1CZyCTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leftmost Derivation\n",
        "import nltk\n",
        "from nltk import CFG\n",
        "from nltk import ChartParser\n",
        "\n",
        "# Define the grammar\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S  -> E\n",
        "  E  -> E '+' T | T\n",
        "  T  -> T '*' F | F\n",
        "  F  -> '(' E ')' | 'id'\n",
        "\"\"\")\n",
        "\n",
        "# Define the sentence\n",
        "sentence = ['id', '+', 'id', '*', 'id']\n",
        "\n",
        "# Create a parser with the given grammar\n",
        "parser = ChartParser(grammar)\n",
        "\n",
        "# Function to generate leftmost derivation\n",
        "def leftmost_derivation(sentence, grammar):\n",
        "    parser = ChartParser(grammar)\n",
        "    trees = list(parser.parse(sentence))\n",
        "    if not trees:\n",
        "        return []\n",
        "\n",
        "    # Take the first parse tree\n",
        "    tree = trees[0]\n",
        "\n",
        "    # Collect derivation steps\n",
        "    derivation_steps = []\n",
        "\n",
        "    def derivation_from_tree(tree):\n",
        "        # Convert tree to string format and add to steps\n",
        "        derivation_steps.append(\" \".join(tree.leaves()))\n",
        "        for production in tree.productions():\n",
        "            lhs = str(production.lhs())\n",
        "            rhs = \" \".join(str(x) for x in production.rhs())\n",
        "            derivation_steps.append(f\"{lhs} -> {rhs}\")\n",
        "        if tree.label() == 'S':\n",
        "            return\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, nltk.Tree) and subtree.label() != 'S':\n",
        "                derivation_from_tree(subtree)\n",
        "\n",
        "    derivation_from_tree(tree)\n",
        "\n",
        "    return derivation_steps\n",
        "\n",
        "# Generate leftmost derivation\n",
        "derivations = leftmost_derivation(sentence, grammar)\n",
        "\n",
        "# Print the derivation steps\n",
        "print(\"Leftmost Derivation Steps:\")\n",
        "for step in derivations:\n",
        "    print(step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9UV8YybwKjy",
        "outputId": "6b35609e-4f6e-464c-93b8-c48db5fcece9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leftmost Derivation Steps:\n",
            "id + id * id\n",
            "S -> E\n",
            "E -> E + T\n",
            "E -> T\n",
            "T -> F\n",
            "F -> id\n",
            "T -> T * F\n",
            "T -> F\n",
            "F -> id\n",
            "F -> id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rightmost Derivation\n",
        "import nltk\n",
        "from nltk import CFG\n",
        "from nltk import ChartParser\n",
        "\n",
        "# Define the grammar\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S  -> E\n",
        "  E  -> E '+' T | T\n",
        "  T  -> T '*' F | F\n",
        "  F  -> '(' E ')' | 'id'\n",
        "\"\"\")\n",
        "\n",
        "# Define the sentence\n",
        "sentence = ['id', '+', 'id', '*', 'id']\n",
        "\n",
        "# Create a parser with the given grammar\n",
        "parser = ChartParser(grammar)\n",
        "\n",
        "# Function to generate leftmost derivation\n",
        "def rightmost_derivation(sentence, grammar):\n",
        "    parser = ChartParser(grammar)\n",
        "    trees = list(parser.parse(sentence))\n",
        "    if not trees:\n",
        "        return []\n",
        "\n",
        "    # Take the first parse tree\n",
        "    tree = trees[0]\n",
        "\n",
        "    # Collect derivation steps\n",
        "    derivation_steps = []\n",
        "\n",
        "    def derivation_from_tree(tree):\n",
        "        # Convert tree to string format and add to steps\n",
        "        derivation_steps.append(\" \".join(tree.leaves()))\n",
        "        for production in tree.productions():\n",
        "            lhs = str(production.lhs())\n",
        "            rhs = \" \".join(str(x) for x in production.rhs())\n",
        "            derivation_steps.append(f\"{lhs} -> {rhs}\")\n",
        "        if tree.label() == 'S':\n",
        "            return\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, nltk.Tree) and subtree.label() != 'S':\n",
        "                derivation_from_tree(subtree)\n",
        "\n",
        "    derivation_from_tree(tree)\n",
        "\n",
        "    return derivation_steps\n",
        "\n",
        "# Generate leftmost derivation\n",
        "derivations = rightmost_derivation(sentence, grammar)\n",
        "\n",
        "# Print the derivation steps\n",
        "print(\"Rightmost Derivation Steps:\")\n",
        "for step in derivations:\n",
        "    print(step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0KOhSXSxkd8",
        "outputId": "2aa8e568-2b9e-450a-fa95-ea9ad5ddf9c1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rightmost Derivation Steps:\n",
            "id + id * id\n",
            "S -> E\n",
            "E -> E + T\n",
            "E -> T\n",
            "T -> F\n",
            "F -> id\n",
            "T -> T * F\n",
            "T -> F\n",
            "F -> id\n",
            "F -> id\n"
          ]
        }
      ]
    }
  ]
}