{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Text Preprocessing\n",
        "Tokenization, Stemming and Lemmatization\n"
      ],
      "metadata": {
        "id": "O_XPApF3XbNW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiP5BJBEVofR",
        "outputId": "d6eb3f23-e761-49a7-a462-ee77fae6d185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'individual', 'words', 'or', 'sentences', '.', 'It', \"'s\", 'a', 'crucial', 'step', 'in', 'NLP', '.']\n",
            "Sentence Tokens: ['Tokenization is the process of breaking down text into individual words or sentences.', \"It's a crucial step in NLP.\"]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"Tokenization is the process of breaking down text into individual words or sentences. It's a crucial step in NLP.\"\n",
        "\n",
        "# Tokenize into words\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", word_tokens)\n",
        "\n",
        "# Tokenize into sentences\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokens:\", sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Initialize stemmers\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Words to stem\n",
        "words = [\"running\", \"ran\", \"runner\", \"easily\", \"fairly\"]\n",
        "\n",
        "# Apply Porter Stemmer\n",
        "porter_stemmed = [porter_stemmer.stem(word) for word in words]\n",
        "print(\"Porter Stemmer Results:\", porter_stemmed)\n",
        "\n",
        "# Apply Snowball Stemmer\n",
        "snowball_stemmed = [snowball_stemmer.stem(word) for word in words]\n",
        "print(\"Snowball Stemmer Results:\", snowball_stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VqtDi0PqKHQ",
        "outputId": "f62847db-42ca-4a4f-bb3c-0675925bf185"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer Results: ['run', 'ran', 'runner', 'easili', 'fairli']\n",
            "Snowball Stemmer Results: ['run', 'ran', 'runner', 'easili', 'fair']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Words to lemmatize\n",
        "words = [(\"running\", \"v\"), (\"better\", \"a\"), (\"cats\", \"n\")]\n",
        "\n",
        "# Lemmatize words with POS tags\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=getattr(wordnet, pos.upper())) for word, pos in words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "dLYSx8pIqZHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords and rarewords removal"
      ],
      "metadata": {
        "id": "8SW0CTMWeNl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk. stem import PorterStemmer\n",
        "from nltk. probability import FreqDist\n",
        "nltk. download ( 'punkt')\n",
        "nltk. download ('stopwords')\n",
        "def preprocess_text(text, rare_threshold=1) :\n",
        "      tokens = word_tokenize (text)\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      filtered_tokens = [token for token in tokens if token. lower () not in stop_words]\n",
        "      frequency_dist = FreqDist(filtered_tokens)\n",
        "      rare_words = set(word for word, freq in frequency_dist. items () if freq <= rare_threshold)\n",
        "      filtered_tokens = [token for token in filtered_tokens if token not in rare_words]\n",
        "      stemmer = PorterStemmer ( )\n",
        "      stemmed_tokens = [stemmer .stem(token) for token in filtered_tokens]\n",
        "      return filtered_tokens, stemmed_tokens\n",
        "def main():\n",
        "     input_text = \"\"\"Your input text goes here. NLP is a machine learning technology.\n",
        "                   Applications of NLP are very useful in real life.\"\"\"\n",
        "     rare_threshold = 1\n",
        "     filtered_tokens, stemmed_tokens = preprocess_text (input_text, rare_threshold)\n",
        "     print(\"Filtered Tokens (after removing stopwords and rare words):\")\n",
        "     print(filtered_tokens)\n",
        "     print(\"\\nStemmed Tokens:\")\n",
        "     print(stemmed_tokens)\n",
        "if __name__==\"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfBco6dYbxHr",
        "outputId": "03eda3e8-d417-4b58-bb44-b0df552dd27e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens (after removing stopwords and rare words):\n",
            "['.', 'NLP', '.', 'NLP', '.']\n",
            "\n",
            "Stemmed Tokens:\n",
            "['.', 'nlp', '.', 'nlp', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-of-Speech (POS) Tagging:\n",
        " Identify the parts of speech"
      ],
      "metadata": {
        "id": "E6AxDMo3gFeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def identify_parts_of_speech(text):\n",
        "   tokens = word_tokenize (text)\n",
        "   pos_tags = nltk.pos_tag (tokens)\n",
        "   return pos_tags\n",
        "def main():\n",
        "   input_text = \"\"\"NLP is a machine learning technology.\n",
        "                Applications of NLP are very useful in real life.\"\"\"\n",
        "   pos_tags = identify_parts_of_speech(input_text)\n",
        "   print (\"Parts of Speech:\")\n",
        "   for token, pos_tag in pos_tags:\n",
        "       print(f'{token}: {pos_tag}')\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIq-bAMyeqBl",
        "outputId": "691f9ec1-aece-4ea9-ef20-95fa2dc0157a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts of Speech:\n",
            "NLP: NNP\n",
            "is: VBZ\n",
            "a: DT\n",
            "machine: NN\n",
            "learning: VBG\n",
            "technology: NN\n",
            ".: .\n",
            "Applications: NNS\n",
            "of: IN\n",
            "NLP: NNP\n",
            "are: VBP\n",
            "very: RB\n",
            "useful: JJ\n",
            "in: IN\n",
            "real: JJ\n",
            "life: NN\n",
            ".: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UnigramTagger, BigramTagger, TrigramTagger"
      ],
      "metadata": {
        "id": "dB8I_5FHgyWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#UnigramTagger, BigramTagger, TrigramTagger\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import UnigramTagger,BigramTagger,TrigramTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "brown_corpus=brown.tagged_sents()\n",
        "train_size=int(0.8*len(brown_corpus))\n",
        "train_set=brown_corpus[:train_size]\n",
        "test_set=brown_corpus[train_size:]\n",
        "\n",
        "unigram_tagger=UnigramTagger(train_set)\n",
        "bigram_tagger=BigramTagger(train_set)\n",
        "trigram_tagger=TrigramTagger(train_set)\n",
        "\n",
        "sentence=\"the quick brown fox jumps over the lazy dog\"\n",
        "tokens=word_tokenize(sentence)\n",
        "\n",
        "unigram_tagged_sentence=unigram_tagger.tag(tokens)\n",
        "bigram_tagged_sentence=bigram_tagger.tag(tokens)\n",
        "trigram_tagged_sentence=trigram_tagger.tag(tokens)\n",
        "print(\"The unigram tagged sentence is \",unigram_tagged_sentence)\n",
        "print(\"The bigram tagged sentence is \",bigram_tagged_sentence)\n",
        "print(\"The trigram tagged sentence is \",trigram_tagged_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbcvX9yKgfQr",
        "outputId": "e1f52380-d685-43ca-abd7-efa7ba5d5bd7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The unigram tagged sentence is  [('the', 'AT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'AT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
            "The bigram tagged sentence is  [('the', 'AT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'AT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
            "The trigram tagged sentence is  [('the', 'AT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', None), ('jumps', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brill Tagger"
      ],
      "metadata": {
        "id": "K9GQh4KcgUHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Brill tagger\n",
        "import nltk\n",
        "from nltk.tag import brill\n",
        "from nltk.tag import UnigramTagger, BrillTaggerTrainer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Sample sentence\n",
        "sentence = \"This is a sample sentence.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Step 1: Prepare data (a single sentence)\n",
        "tagged_sentence = nltk.pos_tag(tokens)\n",
        "\n",
        "# Step 2: Initial tagging\n",
        "baseline_tagger = UnigramTagger([tagged_sentence])\n",
        "\n",
        "# Step 3: Error Analysis\n",
        "initial_tags = baseline_tagger.tag(tokens)\n",
        "print(\"Initial tags:\", initial_tags)\n",
        "\n",
        "# Step 4: Generate Transformation Rules\n",
        "templates = brill.nltkdemo18()\n",
        "\n",
        "# Step 5: Train the Brill Tagger\n",
        "trainer = BrillTaggerTrainer(baseline_tagger, templates, trace=3)\n",
        "brill_tagger = trainer.train([tagged_sentence], max_rules=10)\n",
        "\n",
        "# Step 6: Tag the sentence using Brill Tagger\n",
        "final_tags = brill_tagger.tag(tokens)\n",
        "print(\"Final tags:\", final_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx9_bpfff5RH",
        "outputId": "26f31472-ec9b-408d-b895-f9be8a08b4e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial tags: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n",
            "TBL train (fast) (seqs: 1; tokens: 6; tpls: 18; min score: 2; min acc: None)\n",
            "Finding initial useful rules...\n",
            "    Found 0 useful rules.\n",
            "\n",
            "           B      |\n",
            "   S   F   r   O  |        Score = Fixed - Broken\n",
            "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
            "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
            "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
            "   e   d   n   r  |  e\n",
            "------------------+-------------------------------------------------------\n",
            "Final tags: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affix Tagger"
      ],
      "metadata": {
        "id": "zfZERvG0hbFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Affix tagger\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import AffixTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "brown_corpus=brown.tagged_sents()\n",
        "train_size=int(0.8*len(brown_corpus))\n",
        "train_set=brown_corpus[:train_size]\n",
        "test_set=brown_corpus[train_size:]\n",
        "affix_tagger=AffixTagger(train_set)\n",
        "\n",
        "sentence=\"the quick brown fox jumps over a lazy dog\"\n",
        "tokens=word_tokenize(sentence)\n",
        "tagged_sentence=affix_tagger.tag(tokens)\n",
        "print(\"the tagged sentence is: \",tagged_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5oj2OpPhOuj",
        "outputId": "8733c76e-9da8-4dd9-d0c4-039cc8a7a2b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the tagged sentence is:  [('the', None), ('quick', 'JJ'), ('brown', 'VBN'), ('fox', None), ('jumps', 'NNS'), ('over', None), ('a', None), ('lazy', None), ('dog', None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition (NER):"
      ],
      "metadata": {
        "id": "QvsKXDnRkJh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "class NERTagger:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def tag(self, sentence):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    ne_tagged_tokens = nltk.ne_chunk(tagged_tokens)\n",
        "    return ne_tagged_tokens\n",
        "ner_tagger = NERTagger()\n",
        "\n",
        "sentence = \"Miami is known for its stunning beaches and beautiful skyline views making it a popular tourist destination\"\n",
        "tagged_sentence = ner_tagger.tag(sentence)\n",
        "print(tagged_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXVf9iHLiwS6",
        "outputId": "595b285b-d688-471b-9f26-b7a224e9641c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Miami/NNP)\n",
            "  is/VBZ\n",
            "  known/VBN\n",
            "  for/IN\n",
            "  its/PRP$\n",
            "  stunning/JJ\n",
            "  beaches/NNS\n",
            "  and/CC\n",
            "  beautiful/JJ\n",
            "  skyline/JJ\n",
            "  views/NNS\n",
            "  making/VBG\n",
            "  it/PRP\n",
            "  a/DT\n",
            "  popular/JJ\n",
            "  tourist/NN\n",
            "  destination/NN)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    }
  ]
}